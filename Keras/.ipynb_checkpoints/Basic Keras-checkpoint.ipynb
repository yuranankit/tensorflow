{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sample fully connected nn\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(64, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x22d6546df98>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x22d6546df60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x22d65362ef0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x22d5c65b518>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x22d65429048>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x22d65362dd8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sigmod activation\n",
    "keras.layers.Dense(64, activation='sigmoid')\n",
    "#Or\n",
    "keras.layers.Dense(64, activation=tf.sigmoid)\n",
    "\n",
    "#Layer with L1 regularization to kernel matrix with no activition function\n",
    "keras.layers.Dense(64, kernel_regularizer=keras.regularizers.l1(0.01))\n",
    "#Layer with L2 regularization to bias vector with no activition function\n",
    "keras.layers.Dense(64, bias_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "#Layer with kernel initialized to random orthogonal matrix\n",
    "keras.layers.Dense(64, kernel_initializer='orthogonal')\n",
    "#Layer with bias vector initialized to 2.0\n",
    "keras.layers.Dense(64, bias_initializer=keras.initializers.constant(2.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile model\n",
    "#Compile ex1\n",
    "model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#Compile ex2\n",
    "model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(0.01),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "#Compile ex3\n",
    "model.compile(\n",
    "    optimizer=tf.train.RMSPropOptimizer(0.01),\n",
    "    loss=keras.losses.categorical_crossentropy,\n",
    "    metrics=[keras.metrics.categorical_accuracy]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 202us/step - loss: 11.6686 - categorical_accuracy: 0.0880\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.6129 - categorical_accuracy: 0.0940\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.5991 - categorical_accuracy: 0.0990\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.5960 - categorical_accuracy: 0.1030\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 11.5945 - categorical_accuracy: 0.1000\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.5921 - categorical_accuracy: 0.1110\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.5904 - categorical_accuracy: 0.1040\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.5905 - categorical_accuracy: 0.1170\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 11.5869 - categorical_accuracy: 0.1180\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.5847 - categorical_accuracy: 0.1280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22d654f3240>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample test data\n",
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))\n",
    "\n",
    "model.fit(data,\n",
    "          labels,\n",
    "          epochs=10,\n",
    "          batch_size=32\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 11.5793 - categorical_accuracy: 0.1300 - val_loss: 11.5835 - val_categorical_accuracy: 0.0880\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.5775 - categorical_accuracy: 0.1300 - val_loss: 11.5494 - val_categorical_accuracy: 0.0990\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 53us/step - loss: 11.5698 - categorical_accuracy: 0.1300 - val_loss: 11.6028 - val_categorical_accuracy: 0.0950\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 0s 26us/step - loss: 11.5660 - categorical_accuracy: 0.1470 - val_loss: 11.5526 - val_categorical_accuracy: 0.1010\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.5575 - categorical_accuracy: 0.1500 - val_loss: 11.5578 - val_categorical_accuracy: 0.1010\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.5489 - categorical_accuracy: 0.1470 - val_loss: 11.6341 - val_categorical_accuracy: 0.0880\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.5519 - categorical_accuracy: 0.1480 - val_loss: 11.5732 - val_categorical_accuracy: 0.1240\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.5397 - categorical_accuracy: 0.1600 - val_loss: 11.6315 - val_categorical_accuracy: 0.0960\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.5416 - categorical_accuracy: 0.1700 - val_loss: 11.5630 - val_categorical_accuracy: 0.1130\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 34us/step - loss: 11.5284 - categorical_accuracy: 0.1820 - val_loss: 11.5925 - val_categorical_accuracy: 0.1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22d65852e80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      "800/800 [==============================] - 0s 39us/step - loss: 11.5133 - categorical_accuracy: 0.1850 - val_loss: 11.5855 - val_categorical_accuracy: 0.1100\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 0s 59us/step - loss: 11.4992 - categorical_accuracy: 0.1988 - val_loss: 11.5913 - val_categorical_accuracy: 0.1100\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 0s 39us/step - loss: 11.4955 - categorical_accuracy: 0.1862 - val_loss: 11.5892 - val_categorical_accuracy: 0.1350\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 0s 39us/step - loss: 11.4844 - categorical_accuracy: 0.1888 - val_loss: 11.6130 - val_categorical_accuracy: 0.1550\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 0s 39us/step - loss: 11.4767 - categorical_accuracy: 0.1937 - val_loss: 11.6476 - val_categorical_accuracy: 0.1500\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 0s 20us/step - loss: 11.4751 - categorical_accuracy: 0.2038 - val_loss: 11.6237 - val_categorical_accuracy: 0.1850\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 0s 39us/step - loss: 11.4748 - categorical_accuracy: 0.1738 - val_loss: 11.6349 - val_categorical_accuracy: 0.1400\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 0s 39us/step - loss: 11.4600 - categorical_accuracy: 0.2175 - val_loss: 11.6525 - val_categorical_accuracy: 0.1700\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 0s 39us/step - loss: 11.4527 - categorical_accuracy: 0.2050 - val_loss: 11.6416 - val_categorical_accuracy: 0.1350\n",
      "Epoch 10/10\n",
      "800/800 [==============================] - 0s 20us/step - loss: 11.4477 - categorical_accuracy: 0.2088 - val_loss: 11.6612 - val_categorical_accuracy: 0.1500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22d6534c9b0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sample validation data\n",
    "val_data = np.random.random((1000, 32))\n",
    "val_labels = np.random.random((1000, 10))\n",
    "\n",
    "#Validate with validation data set\n",
    "model.fit(\n",
    "    data,\n",
    "    labels,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(val_data, val_labels)\n",
    ")\n",
    "\n",
    "#Validate with extracted data from training set\n",
    "model.fit(\n",
    "    data,\n",
    "    labels,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 0s 4ms/step - loss: 11.4013 - categorical_accuracy: 0.2740\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.3499 - categorical_accuracy: 0.2583\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.4134 - categorical_accuracy: 0.2760\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 0s 521us/step - loss: 11.3952 - categorical_accuracy: 0.2615\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.3717 - categorical_accuracy: 0.2583\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.4006 - categorical_accuracy: 0.2719\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.3867 - categorical_accuracy: 0.2833\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.3570 - categorical_accuracy: 0.2771\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 0s 521us/step - loss: 11.3741 - categorical_accuracy: 0.2760\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 0s 521us/step - loss: 11.3827 - categorical_accuracy: 0.2917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22d668ae7f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Instantiate dataset from datset api\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "#Train dataset\n",
    "model.fit(\n",
    "    dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30/30 [==============================] - 0s 5ms/step - loss: 11.3691 - categorical_accuracy: 0.2708 - val_loss: 12.1409 - val_categorical_accuracy: 0.1042\n",
      "Epoch 2/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.3165 - categorical_accuracy: 0.3031 - val_loss: 11.6592 - val_categorical_accuracy: 0.1042\n",
      "Epoch 3/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.3866 - categorical_accuracy: 0.2917 - val_loss: 12.2518 - val_categorical_accuracy: 0.1146\n",
      "Epoch 4/10\n",
      "30/30 [==============================] - 0s 521us/step - loss: 11.3744 - categorical_accuracy: 0.2719 - val_loss: 11.6498 - val_categorical_accuracy: 0.1146\n",
      "Epoch 5/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.3519 - categorical_accuracy: 0.2698 - val_loss: 11.7184 - val_categorical_accuracy: 0.0729\n",
      "Epoch 6/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.3746 - categorical_accuracy: 0.2708 - val_loss: 11.6618 - val_categorical_accuracy: 0.1458\n",
      "Epoch 7/10\n",
      "30/30 [==============================] - 0s 521us/step - loss: 11.3530 - categorical_accuracy: 0.2823 - val_loss: 11.9635 - val_categorical_accuracy: 0.0625\n",
      "Epoch 8/10\n",
      "30/30 [==============================] - 0s 521us/step - loss: 11.3419 - categorical_accuracy: 0.2937 - val_loss: 11.9282 - val_categorical_accuracy: 0.0938\n",
      "Epoch 9/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.3578 - categorical_accuracy: 0.2781 - val_loss: 11.7962 - val_categorical_accuracy: 0.1146\n",
      "Epoch 10/10\n",
      "30/30 [==============================] - 0s 1ms/step - loss: 11.3588 - categorical_accuracy: 0.2937 - val_loss: 11.5683 - val_categorical_accuracy: 0.0938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22d66866a90>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validation\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "dataset = dataset.batch(32).repeat()\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels))\n",
    "val_dataset = val_dataset.batch(32).repeat()\n",
    "\n",
    "model.fit(\n",
    "    dataset,\n",
    "    epochs=10,\n",
    "    steps_per_epoch=30,\n",
    "    validation_data=val_dataset,\n",
    "    validation_steps=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 47us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11.401340942382813, 0.237]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11.39579496383667, 0.23854166666666668]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluate with numpy\n",
    "model.evaluate(data, labels, batch_size=32)\n",
    "\n",
    "#Evaluate with dataset\n",
    "model.evaluate(dataset, steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10663608, 0.09594047, 0.08855323, ..., 0.09757347, 0.08840664,\n",
       "        0.09777446],\n",
       "       [0.08736072, 0.07796854, 0.05107785, ..., 0.11174908, 0.02266649,\n",
       "        0.1852497 ],\n",
       "       [0.07738395, 0.11276311, 0.05113854, ..., 0.11724196, 0.16959184,\n",
       "        0.08144818],\n",
       "       ...,\n",
       "       [0.04759455, 0.11217394, 0.09875361, ..., 0.11914393, 0.10998728,\n",
       "        0.06697843],\n",
       "       [0.06597605, 0.21052714, 0.10443663, ..., 0.10773152, 0.05305978,\n",
       "        0.14408375],\n",
       "       [0.10140029, 0.07236691, 0.12146257, ..., 0.08988836, 0.09811846,\n",
       "        0.13266075]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([[0.10663608, 0.09594047, 0.08855323, ..., 0.09757347, 0.08840664,\n",
       "        0.09777446],\n",
       "       [0.08736072, 0.07796854, 0.05107785, ..., 0.11174908, 0.02266649,\n",
       "        0.1852497 ],\n",
       "       [0.07738395, 0.11276311, 0.05113854, ..., 0.11724196, 0.16959184,\n",
       "        0.08144818],\n",
       "       ...,\n",
       "       [0.03832668, 0.06804596, 0.10454365, ..., 0.05316143, 0.11117822,\n",
       "        0.05297511],\n",
       "       [0.1312303 , 0.11090519, 0.04948302, ..., 0.10760076, 0.13704158,\n",
       "        0.14770462],\n",
       "       [0.04703806, 0.10391939, 0.12144806, ..., 0.03398644, 0.18457817,\n",
       "        0.07118084]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict with numpy\n",
    "model.predict(data, batch_size=32)\n",
    "\n",
    "#Predict with dataset\n",
    "model.predict(dataset, steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 6,922\n",
      "Trainable params: 6,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 234us/step - loss: 11.7388 - acc: 0.0910\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 11.6477 - acc: 0.0990\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.6225 - acc: 0.1110\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.6068 - acc: 0.1120\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.5966 - acc: 0.1010\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22d689299e8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model from Functional API\n",
    "inputs = keras.Input(shape=(32, ))\n",
    "\n",
    "#Layer instance is callable on a tensor and returns a tensor\n",
    "x = keras.layers.Dense(64, activation='relu')(inputs)\n",
    "x = keras.layers.Dense(64, activation='relu')(x)\n",
    "predictions = keras.layers.Dense(10, activation='softmax')(x)\n",
    "\n",
    "#Instantiate model\n",
    "model = keras.Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "#Compile model\n",
    "model.compile(\n",
    "    optimizer=tf.train.RMSPropOptimizer(0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#Train model\n",
    "model.fit(\n",
    "    data,\n",
    "    labels,\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 203us/step - loss: 11.6843 - acc: 0.0950\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 11.6580 - acc: 0.0980\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.6227 - acc: 0.1090\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.6053 - acc: 0.1070\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 11.5977 - acc: 0.1130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22d68998c50>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model subclassing\n",
    "class MyModel(keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        #Define layers\n",
    "        self.dense_1 = keras.layers.Dense(32, activation='relu')\n",
    "        self.dense_2 = keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        #Define forward pass\n",
    "        #Using layers in __init__\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)\n",
    "    \n",
    "    def compute_output_shapre(self, input_shape):\n",
    "        #Override for use as part of a functional-style model\n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        shape[-1] = self.num_classes\n",
    "        return tf.TensorShape(shape)\n",
    "    \n",
    "#INstantiates subclassed model\n",
    "model = MyModel(num_classes=10)\n",
    "\n",
    "#Compile subclassed model\n",
    "model.compile(\n",
    "    optimizer=tf.train.RMSPropOptimizer(0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#Fit model\n",
    "model.fit(\n",
    "    data,\n",
    "    labels,\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 188us/step - loss: 13.2316 - acc: 0.0890\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 12.5370 - acc: 0.0910\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 12.1519 - acc: 0.0960\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 12.0456 - acc: 0.0950\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 12.0114 - acc: 0.0970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22d68fa6748>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "my_layer_3 (MyLayer)         (None, 10)                320       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 320\n",
      "Trainable params: 320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Custom Layers\n",
    "class MyLayer(keras.layers.Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        shape = tf.TensorShape((input_shape[1], self.output_dim))\n",
    "        #Create a trainable weight variable for this layer\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                      shape=shape,\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=True\n",
    "                                     )\n",
    "        super(MyLayer, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.kernel)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        shape = tf.TensorShape(input_shape).as_list()\n",
    "        shape[-1] = self.output_dim\n",
    "        return tf.TensorShape(shape)\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super(MyLayer, self).get_config()\n",
    "        base_config['output_dim'] = self.output_dim\n",
    "        \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "    \n",
    "#Create model with custom layer\n",
    "model = keras.Sequential([\n",
    "    MyLayer(10),\n",
    "    keras.layers.Activation('softmax')\n",
    "])\n",
    "\n",
    "#Compile\n",
    "model.compile(\n",
    "    optimizer=tf.train.RMSPropOptimizer(0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#Train\n",
    "model.fit(\n",
    "    data,\n",
    "    labels,\n",
    "    epochs=5,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/50\n",
      "1000/1000 [==============================] - 0s 36us/step - loss: 11.6645 - acc: 0.1050 - val_loss: 11.6247 - val_acc: 0.0980\n",
      "Epoch 2/50\n",
      "1000/1000 [==============================] - 0s 35us/step - loss: 11.6575 - acc: 0.1020 - val_loss: 11.6177 - val_acc: 0.1040\n",
      "Epoch 3/50\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.6504 - acc: 0.1010 - val_loss: 11.6126 - val_acc: 0.1050\n",
      "Epoch 4/50\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.6448 - acc: 0.1090 - val_loss: 11.6075 - val_acc: 0.0980\n",
      "Epoch 5/50\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.6381 - acc: 0.0980 - val_loss: 11.6057 - val_acc: 0.0960\n",
      "Epoch 6/50\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.6331 - acc: 0.1000 - val_loss: 11.5994 - val_acc: 0.1060\n",
      "Epoch 7/50\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.6286 - acc: 0.0990 - val_loss: 11.5974 - val_acc: 0.0940\n",
      "Epoch 8/50\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.6242 - acc: 0.1090 - val_loss: 11.5918 - val_acc: 0.1070\n",
      "Epoch 9/50\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.6199 - acc: 0.1080 - val_loss: 11.5879 - val_acc: 0.0960\n",
      "Epoch 10/50\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.6163 - acc: 0.1120 - val_loss: 11.5831 - val_acc: 0.0990\n",
      "Epoch 11/50\n",
      "1000/1000 [==============================] - 0s 31us/step - loss: 11.6124 - acc: 0.1090 - val_loss: 11.5833 - val_acc: 0.1030\n",
      "Epoch 12/50\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.6097 - acc: 0.1090 - val_loss: 11.5795 - val_acc: 0.1040\n",
      "Epoch 13/50\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.6066 - acc: 0.1070 - val_loss: 11.5756 - val_acc: 0.0940\n",
      "Epoch 14/50\n",
      "1000/1000 [==============================] - 0s 37us/step - loss: 11.6039 - acc: 0.1070 - val_loss: 11.5741 - val_acc: 0.0940\n",
      "Epoch 15/50\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 11.6014 - acc: 0.1120 - val_loss: 11.5723 - val_acc: 0.0870\n",
      "Epoch 16/50\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 11.5989 - acc: 0.1140 - val_loss: 11.5717 - val_acc: 0.0930\n",
      "Epoch 17/50\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 11.5969 - acc: 0.1160 - val_loss: 11.5704 - val_acc: 0.0990\n",
      "Epoch 18/50\n",
      "1000/1000 [==============================] - 0s 70us/step - loss: 11.5953 - acc: 0.1090 - val_loss: 11.5681 - val_acc: 0.0890\n",
      "Epoch 19/50\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 11.5937 - acc: 0.1170 - val_loss: 11.5652 - val_acc: 0.0970\n",
      "Epoch 20/50\n",
      "1000/1000 [==============================] - 0s 81us/step - loss: 11.5916 - acc: 0.1110 - val_loss: 11.5649 - val_acc: 0.0990\n",
      "Epoch 21/50\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 11.5903 - acc: 0.1190 - val_loss: 11.5657 - val_acc: 0.1040\n",
      "Epoch 22/50\n",
      "1000/1000 [==============================] - 0s 47us/step - loss: 11.5884 - acc: 0.1140 - val_loss: 11.5610 - val_acc: 0.1090\n",
      "Epoch 23/50\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 11.5874 - acc: 0.1120 - val_loss: 11.5630 - val_acc: 0.1010\n",
      "Epoch 24/50\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 11.5859 - acc: 0.1130 - val_loss: 11.5609 - val_acc: 0.0980\n",
      "Epoch 25/50\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 11.5852 - acc: 0.1190 - val_loss: 11.5602 - val_acc: 0.0960\n",
      "Epoch 26/50\n",
      "1000/1000 [==============================] - 0s 62us/step - loss: 11.5841 - acc: 0.1230 - val_loss: 11.5591 - val_acc: 0.0920\n",
      "Epoch 27/50\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 11.5836 - acc: 0.1090 - val_loss: 11.5577 - val_acc: 0.0970\n",
      "Epoch 28/50\n",
      "1000/1000 [==============================] - 0s 78us/step - loss: 11.5819 - acc: 0.1140 - val_loss: 11.5593 - val_acc: 0.0790\n",
      "Epoch 29/50\n",
      "1000/1000 [==============================] - 0s 94us/step - loss: 11.5817 - acc: 0.1130 - val_loss: 11.5602 - val_acc: 0.0940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22d69196128>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Callback\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=2, monitor='val_loss'),\n",
    "    keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    data,\n",
    "    labels,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(val_data, val_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parent directory of my_model_weight doesn't exist, can't save.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a directory: ; No such file or directory\n\t [[Node: save_8/SaveV2 = SaveV2[dtypes=[DT_STRING, DT_STRING, DT_STRING, DT_STRING, DT_FLOAT, DT_FLOAT, DT_STRING, DT_FLOAT, DT_FLOAT, DT_STRING, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_8/Const_0_6, save_8/SaveV2/tensor_names, save_8/SaveV2/shape_and_slices, _arg_Const_60_0_0, _arg_Const_65_0_5, _arg_Const_61_0_1, _arg_Const_62_0_2, dense_65/bias/Read/ReadVariableOp, dense_65/kernel/Read/ReadVariableOp, _arg_Const_63_0_3, dense_66/bias/Read/ReadVariableOp, dense_66/kernel/Read/ReadVariableOp, _arg_Const_64_0_4, dense_67/bias/Read/ReadVariableOp, dense_67/kernel/Read/ReadVariableOp)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1651\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_tensor_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1652\u001b[1;33m               {self.saver_def.filename_tensor_name: checkpoint_file})\n\u001b[0m\u001b[0;32m   1653\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\util.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, **kwargs)\u001b[0m\n\u001b[0;32m    836\u001b[0m     return self._wrapped_session.run(\n\u001b[1;32m--> 837\u001b[1;33m         fetches=fetches, feed_dict=feed_dict, **kwargs)\n\u001b[0m\u001b[0;32m    838\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Failed to create a directory: ; No such file or directory\n\t [[Node: save_8/SaveV2 = SaveV2[dtypes=[DT_STRING, DT_STRING, DT_STRING, DT_STRING, DT_FLOAT, DT_FLOAT, DT_STRING, DT_FLOAT, DT_FLOAT, DT_STRING, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_8/Const_0_6, save_8/SaveV2/tensor_names, save_8/SaveV2/shape_and_slices, _arg_Const_60_0_0, _arg_Const_65_0_5, _arg_Const_61_0_1, _arg_Const_62_0_2, dense_65/bias/Read/ReadVariableOp, dense_65/kernel/Read/ReadVariableOp, _arg_Const_63_0_3, dense_66/bias/Read/ReadVariableOp, dense_66/kernel/Read/ReadVariableOp, _arg_Const_64_0_4, dense_67/bias/Read/ReadVariableOp, dense_67/kernel/Read/ReadVariableOp)]]\n\nCaused by op 'save_8/SaveV2', defined at:\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\asyncio\\events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 117, in _handle_events\n    handler_func(fileobj, events)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2909, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-55-25dc78d5f490>\", line 15, in <module>\n    model.save_weights('my_model_weight')\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\", line 1308, in save_weights\n    self._checkpointable_saver.save(filepath, session=session)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\util.py\", line 968, in save\n    self._last_save_saver = saver_lib.Saver(var_list=named_variables)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1284, in __init__\n    self.build()\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1296, in build\n    self._build(self._filename, build_save=True, build_restore=True)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 1333, in _build\n    build_save=build_save, build_restore=build_restore)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 778, in _build_internal\n    save_tensor = self._AddSaveOps(filename_tensor, saveables)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 278, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\", line 194, in save_op\n    tensors)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1800, in save_v2\n    shape_and_slices=shape_and_slices, tensors=tensors, name=name)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3414, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\yuranan\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1740, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Failed to create a directory: ; No such file or directory\n\t [[Node: save_8/SaveV2 = SaveV2[dtypes=[DT_STRING, DT_STRING, DT_STRING, DT_STRING, DT_FLOAT, DT_FLOAT, DT_STRING, DT_FLOAT, DT_FLOAT, DT_STRING, DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](_arg_save_8/Const_0_6, save_8/SaveV2/tensor_names, save_8/SaveV2/shape_and_slices, _arg_Const_60_0_0, _arg_Const_65_0_5, _arg_Const_61_0_1, _arg_Const_62_0_2, dense_65/bias/Read/ReadVariableOp, dense_65/kernel/Read/ReadVariableOp, _arg_Const_63_0_3, dense_66/bias/Read/ReadVariableOp, dense_66/kernel/Read/ReadVariableOp, _arg_Const_64_0_4, dense_67/bias/Read/ReadVariableOp, dense_67/kernel/Read/ReadVariableOp)]]\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-25dc78d5f490>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#Save model weights by checkpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'my_model_weight'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m#Restore model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36msave_weights\u001b[1;34m(self, filepath, overwrite, save_format)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1307\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1308\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpointable_saver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1310\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\checkpointable\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, file_prefix, checkpoint_number, session)\u001b[0m\n\u001b[0;32m    974\u001b[0m           \u001b[0msave_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[0mwrite_meta_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 976\u001b[1;33m           global_step=checkpoint_number)\n\u001b[0m\u001b[0;32m    977\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1667\u001b[0m               \"Parent directory of {} doesn't exist, can't save.\".format(\n\u001b[0;32m   1668\u001b[0m                   save_path))\n\u001b[1;32m-> 1669\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwrite_meta_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Parent directory of my_model_weight doesn't exist, can't save."
     ]
    }
   ],
   "source": [
    "#Create sample model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='rmsprop',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "#Save model weights by checkpoint\n",
    "model.save_weights('./my_model_weight')\n",
    "\n",
    "#Restore model\n",
    "model.load_weights('./my_model_weight')\n",
    "\n",
    "#Save model weights in HDF5 format\n",
    "model.save_weights('./my_model_weight.h5', save_format='h5')\n",
    "\n",
    "#Restore model\n",
    "model.load_weights('./my_model_weight.h5')\n",
    "\n",
    "model.summary()\n",
    "\n",
    "#Save entire model to HDF50\n",
    "model.save('my_model.h5')\n",
    "\n",
    "#Recreate\n",
    "model = keras.models.load_model('my_model.h5')\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
